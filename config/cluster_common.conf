################################## User Client Configuration ##################################
#where is the hadoop-client
hadoop_client_home=/root/tool/output/hadoop-client/hadoop
#where is the hpc_client
hpc_client_home=/home/tool/hpc_client


################################## Data Configuration ##################################
#attention: files for training should be put on hdfs
##the list contains all file locations should be specified here
fs_name=hdfs://szwg-ecomon-hdfs.dmop.baidu.com:54310
##If force_reuse_output_path is True ,paddle will remove output_path without check output_path exist
force_reuse_output_path=True
##ugi of hdfs
fs_ugi=gushitong,gushitong2015
##train data path on hdfs
train_data_path="/app/insight/EBG/gushitong/xutao/000001same/train/data/*"
##test data path on hdfs, can be null or not setted
test_data_path="/app/insight/EBG/gushitong/xutao/000001same/validate/data/*"
##train meta data path, can be null or not setted
train_meta_data_path=
##test meta data path, can be null or not setted
test_meta_data_path=
#the output directory on hdfs
output_path="/app/insight/EBG/gushitong/xutao/000001same/train/output_`date +%Y%m%d%H%M%S`"
#the model path on hdfs when evaluation
model_path=
#the initial model path on hdfs used to init parameters
init_model_path=


#the initial model path for pservers
pserver_model_dir=
#which pass
pserver_model_pass=
#example of above 2 args:
#if set pserver_model_dir to /app/paddle/models
#and set pserver_model_pass to 123
#then rank 0 will download model from /app/paddle/models/rank-00000/pass-00123/
#and rank 1 will download model from /app/paddle/models/rank-00001/pass-00123/, etc.

#save_dir for paddle_trainer. if set to other directory other than "output", the models WILL NOT BE put to hdfs.
save_dir=output

#save model by pserver (default is 0, that is by trainer. 1 is by pserver)
loadsave_parameters_in_pserver=0

#In evaluation mode, the user can obtain the output values of layers, which are specified by 
#the Outputs function in trainer_config.conf
enable_predict_output=0

################################## MPI Configuration ##################################
SERVER=wg-hpc-10g.dmop.baidu.com
QUEUE=dpf
PRIORITY=very_high
#the job name on mpi cluster
mpi_job_name=rnn_c_`date +%Y%m%d%H%M%S`
#the maximum running time of the job
mpi_wall_time=100:30:00
#how many mpi nodes will be used. nodes will be assigned by cluster automatically.
mpi_nodes=3
#[special usage]user can specify node names like the following.
#in this case, the job will be executed on exactly certain nodes. if any of the nodes is used or down, the job will be queued.
#the advantage is, user can download training instances only once ( to a directory other than ./ ).
#mpi_nodes=m1-idl-gpu2-bak15.m1.baidu.com+m1-idl-gpu2-bak14.m1.baidu.com+m1-idl-gpu2-bak13.m1.baidu.com+m1-idl-gpu2-bak12.m1.baidu.com+m1-idl-gpu2-bak11.m1.baidu.com+m1-idl-gpu2-bak10.m1.baidu.com+m1-idl-gpu2-bak09.m1.baidu.com+m1-idl-gpu2-bak07.m1.baidu.com+m1-idl-gpu2-bak05.m1.baidu.com+m1-idl-gpu2-bak04.m1.baidu.com


################################## Paddle program arguments Configuration ##################################
#the 'comment' for pserver and trainer
comment="${PBS_O_LOGNAME}_${PBS_JOBID}"
#the parameters for pserver
server_arg="--port=7164 --ports_num=1"

#the "pserver for test" will be used only if test_server_arg is not empty
#example: test_server_arg="--port=17164 --ports_num=1 --comment=$comment" (use a DIFFERENT port from server_arg!)
test_server_arg=

#the parameters for trainer
#attention: do not specify args of --trainer_id and --pservers. those two parameters will be set automatically.
train_arg="--saving_period=50 --port=7164 --ports_num=1 --local=0 --comment=$comment --dot_period=1000000 --log_period=100 --num_passes=2000 --trainer_count=10 --show_parameter_stats_period=1000000"
#the parameters for tester
test_arg="--port=7164 --ports_num=1 --distribute_test=0 --job=test --test_pass=0 --test_wait=50 --dot_period=100000 --log_period=100000 --saving_period=50 --num_passes=2000 --start_pserver=0"


################################## Network Card Configuration ##################################
#Nics' device name config
nics="xgbe0"
#Nics Strategy for IP address :
#0 : only one nic;
#1 : nics >= 2, ip&0.0.255.0 - 1;
#2 : nics >= 2, ip&0.0.0.255 + 128.
nics_strategy=0

#use_40G_network_card=1 && rdma_tcp=tcp: run tcp on 40G network card
#use_40G_network_card=1 && rdma_tcp=rdma: run rdma on 40G network card
#use_40G_network_card=0: run tcp on 10G network card (default)
use_40G_network_card=0
rdma_tcp="tcp"  
if [ "$use_40G_network_card" -ne 0 ]; then 
    nics="eth2.100"
fi

################################## Cluster Performance Configuration ######################
#if slow node exist, send mail to user
mail_address=

################################## Download Configuration ##################################
#whether to download file, set to 1 means download, set to 0 means 'generate file list only'.
real_download=1
#where to save downloaded file. must be ended with a '/'.
#if set to './', files will be placed to local dir of mpi job, and be removed when job finished.
#if set to other directory out of ./, the files WILL NOT BE REMOVED and may OCCUPY A LOT OF DISK until someone delete them manually.
download_destination=./train_data_dir/
test_download_destination=./test_data_dir/
#local directory name and file name is a suffix of original hdfs name.
#for example, a file located on '/app/inf/mpi/houjue/7WData/7WFeature/1-10000feat.join',
#             download_destination is './',
#             subdir_level is '2'
#then the final local file will be './7WData/7WFeature/1-10000feat.join'
subdir_level=1
#the files in filelist will be 'grouped' by group_size. each rank will download files by 'group'.
group_size=1

#When HDFS is mounted on the machine as a standard file system, the user can access data directly
#via hadoop vfs. In this case, train_data_path and test_data_path in Data Configuration
#must be the mounted path. The default option is not to use hadoop vfs
use_hadoop_vfs=False

#Specify the number of threads for downloading files from HDFS
download_thread_num=15

################################## Python Env ##################################
#The default python version in Baidu machines may different, the version may
#be 2.4 2.6 or 2.7 etc. So we always use our own python to make our python
#command run correctly.
if [ -d ./python-gcc345 ]; then
    python_cmd="PYTHONHOME=./python-gcc345 \
               LD_LIBRARY_PATH=./python-gcc345/lib:$LD_LIBRARY_PATH \
               ./python-gcc345/bin/python"
fi

use_gcc48_and_python=0


# nvidia_smi interval time, 0 will not print.
nvidia_smi_loop_time=60
