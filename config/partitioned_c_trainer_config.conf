
################################### Data Configuration ###################################
TrainData(ProtoData(files = "train.list", type = "proto_sequence"))

################################### Algorithm Configuration ###################################
Settings(
    learning_rate_decay_a = 1e-05,
    learning_rate_decay_b = 0.0,
    learning_rate = 0.001,
    batch_size = 128,
    num_batches_per_get_parameter = 1,
    algorithm = 'async_sgd',
    num_batches_per_send_parameter = 1,
    learning_method = 'rmsprop',
)

feature_types = 16
period = 20

################################### Network Configuration ###################################
Layer(type = "data", name = "input", size = feature_types * period)
for ix in range(feature_types):
    Layer(name = 'layer0_%s' % ix, type = 'subseq', bias = False, inputs = ['input', "%s" % (ix * period), "%s" % (period)])
    Layer(inputs = [Input("layer0_%s" % (ix)), parameter_name = "_layer1_%s.w" % ix], name = "layer1_%s" % ix, bias = Bias(parameter_name = "_layer1_%s.bias" % ix), active_type = "brelu", type = "fc", size = period/2)
Layer(inputs = [Input('layer1_%s' % ix) for x in range(feature_types)], name = 'concate_layer1', active_type = "", type = "concat")
Layer(inputs = [Input("concate_layer1", parameter_name = "_layer2.w")], name = "layer2", bias = Bias(parameter_name = "_layer2.bias"), active_type = "brelu", type = "fc", size = feature * period / 4)
Layer(inputs = [Input("layer2", parameter_name = "_output.w")], name = "output", bias = Bias(parameter_name = "_output.bias"), active_type = "softmax", type = "fc", size = 2)
Layer(type = "data", name = "value", size = 1)
Layer(type = "data", name = "label", size = 1)
Layer(inputs = [Input("output"), Input("label")], type = "multi-class-cross-entropy", name = "cost")
Evaluator(inputs = ["output", "label"], type = "classification_error", name = "classification_error")
Inputs("input", "value", "label")
Outputs("cost")