#edit-mode: -*- python -*-
#coding:gbk

#default_initial_smart(True)

input_dim = 20
cols_dim = 1

model_type("nn")

TrainData(ProtoData(        
            type = "proto_sequence",
            files = ('train.list')
            ))

Settings(
    learning_rate=3e-3,
    batch_size=128,
    algorithm='async_sgd',
    learning_method='adagrad',
    ada_epsilon=0.001,
    ada_rou=0.95,
)


flayer1_dim = 10

Inputs("input", "value", "label")

Layer(
    name = "input",
    type = "data",
    size = input_dim
)

Layer(
    name = "value",
    type = "data",
    size = 1
)

Layer(
    name = "label",
    type = "data",
    size = 2,
)


Layer(
    name = "reshape",
    type = "seqreshape",
    size = cols_dim,
    bias = False,
    inputs = [Input("input")]
)

Layer(
    name = "lstm_input_transformed1",
    type = "mixed",
    size = cols_dim * 4,
    bias = False,
    inputs = [
        FullMatrixProjection("reshape", initial_smart=True),
    ]
)

Layer(
    name = "lstm1",
    type = "lstmemory",
    active_type = "tanh",
    active_state_type = "tanh",
    active_gate_type = "sigmoid",
    bias = Bias(initial_std=0),
    inputs = Input("lstm_input_transformed1", initial_std=0, learning_rate=1),
)


# only output the last timestep of the input
#Layer(
    #name = "rnnlast",
    #type = "seqlastins",
    #inputs = ["rnn1"],
#)

Layer(
    name = "flayer1",
    type = "fc",
    active_type = "tanh",
    size = flayer1_dim,
    bias = True,
    inputs = Input("lstm1")
)

Layer(
    name = "output",
    type = "fc",
    size = 2,
    active_type = "softmax",
    bias = True,
    inputs = Input("flayer1")
)

Outputs("output")