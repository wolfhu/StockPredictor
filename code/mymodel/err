Using gpu device 0: Tesla K40c (CNMeM is disabled, cuDNN 4007)
Traceback (most recent call last):
  File "dnn.py", line 402, in <module>
    run_dnn(learning_rate, dnn_strategy, possitive_punishment)
  File "dnn.py", line 321, in run_dnn
    err, acc= train(inputs, targets)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 875, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python2.7/dist-packages/theano/gof/link.py", line 317, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 862, in __call__
    self.fn() if output_subset is None else\
ValueError: GpuReshape: cannot reshape input of shape (128, 1, 20) to shape (128, 16, 20).
Apply node that caused the error: GpuReshape{3}(GpuFromHost.0, MakeVector{dtype='int64'}.0)
Toposort index: 115
Inputs types: [CudaNdarrayType(float32, 3D), TensorType(int64, vector)]
Inputs shapes: [(128, 1, 20), (3,)]
Inputs strides: [(20, 0, 1), (8,)]
Inputs values: ['not shown', array([128,  16,  20])]
Outputs clients: [[GpuDimShuffle{0,1,x,2}(GpuReshape{3}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
